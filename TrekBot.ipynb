{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Welcome to the TrekBot Colab notebook! This notebook behaves exactly like a jupyter notebook. Let's dive right in!\n",
        "\n",
        "First step: put the JSON data file here inside a folder called \"data\":\n",
        "https://www.kaggle.com/datasets/gjbroughton/start-trek-scripts?resource=download\n",
        "\n",
        "Now we'll install a few necessary packages."
      ],
      "metadata": {
        "id": "WJki-8BTOjQj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDa-3B1j6RXn"
      },
      "outputs": [],
      "source": [
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install scikit-learn\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we import the packages we'll need.\n",
        "\n",
        "\n",
        "**pandas** is a package that allows us to conveniently store and manipulate data in a data structure known as a Dataframe. (This is similar to a Dataframe in R, for those familiar with R.) It’s a very common tool for anyone doing data science in python.\n",
        "\n",
        "**sklearn** is the package formally called “scikit-learn”, and contains a wide range of statistical and machine learning methods. It’s another very common package for data scientists in python.\n",
        "\n",
        "**numpy** is python’s main numeric library, and allows us to do things like work with arrays, matrices, dot products, etc.\n",
        "\n",
        "**json** is a package for interacting with json files. Our data is formatted as a single json file, so this is useful for us here.\n",
        "\n",
        "**os** helps us with file management and command-line commands.\n",
        "\n",
        "**openai** is a package containing functions that allow us to easily make API calls to OpenAI’s models in python.\n",
        "\n",
        "Finally, we import **cosine_similarity** from sklearn, since it’s a specialized function that we need today."
      ],
      "metadata": {
        "id": "BuN-ORGzYP5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import openai\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "\n",
        "CHUNK_SIZE = 600\n",
        "OVERLAP = 20"
      ],
      "metadata": {
        "id": "nv9fSpa1-SmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember the OpenAI API key you created? Copy and paste it in the cell below."
      ],
      "metadata": {
        "id": "i73a2SiNYWz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = input(\"Paste your OpenAI API key here and hit enter:\")"
      ],
      "metadata": {
        "id": "9YbnFGeS-e5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's what the model is doing: we have a long piece of text that we want ChatGPT to be able to answer questions about. We first break that text up into chunks containing 600 words (technically called “tokens”), where each chunk overlaps 20 words with the following chunk. We then send these chunks to OpenAI to obtain their embeddings. When we ask a question about our text, we find the question’s embedding, and use cosine similarity to find the chunk of text that is closest to our question. We then send a query to ChatGPT that includes our original question, as well as the chunk of text as context.\n",
        "\n",
        "We loop over all the chunks, and send each one to OpenAI, get back the embedding, and then write a new line to the Dataframe df. Note that we are casting the embedding response (a string) to a numpy array. We do this because we will be doing numerical operations on the embedding in just a moment."
      ],
      "metadata": {
        "id": "qq0Z-KFYYbuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scripts = json.load(open(\"data/all_scripts_raw.json\", encoding='ascii')) # https://www.kaggle.com/datasets/gjbroughton/start-trek-scripts?resource=download\n",
        "text = scripts['TNG']['episode 99']\n",
        "text_list = text.split()\n",
        "chunks = [text_list[i:i+CHUNK_SIZE] for i in range(0, len(text_list), CHUNK_SIZE-OVERLAP)]\n",
        "df = pd.DataFrame(columns=['chunk', 'gpt_raw', 'embedding'])\n",
        "for chunk in chunks:\n",
        "    f = openai.Embedding.create(\n",
        "        model=\"text-embedding-ada-002\",\n",
        "        input=\" \".join(chunk),\n",
        "    )\n",
        "    df.loc[len(df.index)] = (chunk, f, np.array(f['data'][0]['embedding']))"
      ],
      "metadata": {
        "id": "nR3icJ14_Di0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "fh6mCPlNOtgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let’s define our query and get its embedding. Our query is a simple question: who was the captain of the Excalibur? A bit of context: in this episode, a small detail is that one of the crew members was assigned to command a ship for this one episode only, and it’s a minor detail in the plot of the episode. In fact, if you ask ChatGPT this question without giving it the script, it doesn’t know the answer. We’ll see that with the right chunk of text, identified by cosine similarity, ChatGPT can answer correctly.\n",
        "\n",
        "We calculate the cosine distance from our query to each chunk, and save the chunk that is most similar to a variable called context_chunk.\n",
        "\n",
        "Finally, we assemble the full query, including the chunk we identified, and send it to ChatGPT via the API:"
      ],
      "metadata": {
        "id": "nOYoYbnYY17L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Who was the captain of the Excalibur?\"\n",
        "f = openai.Embedding.create(\n",
        "    model=\"text-embedding-ada-002\",\n",
        "    input=query\n",
        ")\n",
        "query_embedding = np.array(f['data'][0]['embedding'])\n",
        "\n",
        "similarity = []\n",
        "for arr in df['embedding'].values:\n",
        "    similarity.extend(cosine_similarity(query_embedding.reshape(1, -1), arr.reshape(1, -1)))\n",
        "context_chunk = chunks[np.argmax(similarity)]\n",
        "\n",
        "query_to_send = \"CONTEXT: \" + \" \".join(context_chunk) + \"\\n\\n\" + query\n",
        "response = openai.Completion.create(\n",
        "  model=\"text-davinci-003\",\n",
        "  prompt= query_to_send,\n",
        "  max_tokens=100,\n",
        "  temperature=0\n",
        ")"
      ],
      "metadata": {
        "id": "53974KA0_EuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(query_to_send)"
      ],
      "metadata": {
        "id": "P_a-RUeMQ53O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test our bot. Did it get it right? Execute the cell below to find out!"
      ],
      "metadata": {
        "id": "riOblNiGZA5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(response['choices'][0]['text'].strip())"
      ],
      "metadata": {
        "id": "_i9zCaYV_G9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yEei7bCiBvc6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}